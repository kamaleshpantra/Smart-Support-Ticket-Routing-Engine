{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kamaleshpantra/Synthetic-Image-Detector/blob/main/Synthetic_Image_detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "y9_DtP9HllWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsZA9qWrOlpB",
        "outputId": "ad164ba9-4787-4023-d7db-1e9192d3f594"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "from torchvision import transforms, models\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, Subset, random_split\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    confusion_matrix\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0WmBIX4nOcn",
        "outputId": "0472f7b3-3b8e-4429-867d-d862b6a2c878"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reproducibility"
      ],
      "metadata": {
        "id": "SInVDXOKnXFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "L4aD0SzLnR9y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Project Paths"
      ],
      "metadata": {
        "id": "6IAIyIY0naTr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT_ROOT = \"/content/drive/MyDrive/synthetic-detector\"\n",
        "DATA_PATH = os.path.join(PROJECT_ROOT, \"data\")\n",
        "\n",
        "TRAIN_PATH = os.path.join(DATA_PATH, \"train\")\n",
        "TEST_PATH  = os.path.join(DATA_PATH, \"test\")\n",
        "\n",
        "MODEL_DIR = os.path.join(PROJECT_ROOT, \"outputs/models\")\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Train path:\", TRAIN_PATH)\n",
        "print(\"Test path:\", TEST_PATH)"
      ],
      "metadata": {
        "id": "hVo9VSn4ndon",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ab46047-3062-4ebd-a1da-a1de849e1da2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train path: /content/drive/MyDrive/synthetic-detector/data/train\n",
            "Test path: /content/drive/MyDrive/synthetic-detector/data/test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load CIFAKE Dataset"
      ],
      "metadata": {
        "id": "ZPs5vXvToC0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "train_full_dataset = ImageFolder(root=TRAIN_PATH, transform=transform)\n",
        "test_full_dataset  = ImageFolder(root=TEST_PATH, transform=transform)\n",
        "\n",
        "print(\"Classes:\", train_full_dataset.classes)\n",
        "print(\"Train size:\", len(train_full_dataset))\n",
        "print(\"Test size:\", len(test_full_dataset))"
      ],
      "metadata": {
        "id": "vJtQt_qIoLcE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8b0849a-07a3-4245-b9f2-8e82fd26d066"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: ['FAKE', 'REAL']\n",
            "Train size: 30002\n",
            "Test size: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Balanced Subset"
      ],
      "metadata": {
        "id": "L9q-2WHloM-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "targets = train_full_dataset.targets\n",
        "indices = []\n",
        "\n",
        "samples_per_class = 15000\n",
        "class_counts = {0: 0, 1: 0}\n",
        "\n",
        "for idx, label in enumerate(targets):\n",
        "    if class_counts[label] < samples_per_class:\n",
        "        indices.append(idx)\n",
        "        class_counts[label] += 1\n",
        "\n",
        "    if all(v == samples_per_class for v in class_counts.values()):\n",
        "        break\n",
        "\n",
        "train_subset = Subset(train_full_dataset, indices)\n",
        "\n",
        "print(\"Balanced train subset:\", len(train_subset))\n",
        "print(\"Class distribution:\", class_counts)"
      ],
      "metadata": {
        "id": "z6jgA209oP6M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e97fc5f-875b-428b-cb44-182a62f7f21b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Balanced train subset: 30000\n",
            "Class distribution: {0: 15000, 1: 15000}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets = test_full_dataset.targets\n",
        "indices = []\n",
        "\n",
        "samples_per_class = 5000\n",
        "class_counts = {0: 0, 1: 0}\n",
        "\n",
        "for idx, label in enumerate(targets):\n",
        "    if class_counts[label] < samples_per_class:\n",
        "        indices.append(idx)\n",
        "        class_counts[label] += 1\n",
        "\n",
        "    if all(v == samples_per_class for v in class_counts.values()):\n",
        "        break\n",
        "\n",
        "test_subset = Subset(test_full_dataset, indices)\n",
        "\n",
        "print(\"Balanced test subset:\", len(test_subset))\n",
        "print(\"Class distribution:\", class_counts)"
      ],
      "metadata": {
        "id": "wlboaNcd2VEY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bab7a183-d6ce-480f-ff7f-f3b29d654355"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Balanced test subset: 10000\n",
            "Class distribution: {0: 5000, 1: 5000}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train / Validation Split"
      ],
      "metadata": {
        "id": "jT5es1DfoT8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(0.85 * len(train_subset))\n",
        "val_size = len(train_subset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(\n",
        "    train_subset,\n",
        "    [train_size, val_size]\n",
        ")\n",
        "\n",
        "print(\"Final Train:\", len(train_dataset))\n",
        "print(\"Validation:\", len(val_dataset))"
      ],
      "metadata": {
        "id": "xehHcPbuoWQ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ee4552a-a188-41dd-8083-0a2f529f8de2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Train: 25500\n",
            "Validation: 4500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Loader"
      ],
      "metadata": {
        "id": "NpYpJpOxrqOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "test_loader  = DataLoader(test_full_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "print(\"DataLoaders ready.\")"
      ],
      "metadata": {
        "id": "Z1QX7SpmrrkW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9d667c8-5d86-42b3-d990-fbf6de5a029e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataLoaders ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build ResNet18"
      ],
      "metadata": {
        "id": "SN-iW3AzoYrr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "\n",
        "# Freeze early layers\n",
        "for param in list(model.parameters())[:-10]:\n",
        "    param.requires_grad = False\n",
        "\n",
        "num_features = model.fc.in_features\n",
        "\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(num_features, 512),\n",
        "    nn.BatchNorm1d(512),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.4),\n",
        "    nn.Linear(512, 2)\n",
        ")\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "scaler = GradScaler()\n",
        "\n",
        "print(\"Model ready.\")"
      ],
      "metadata": {
        "id": "e7YzUNQbociE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c03a510-1905-4035-8e7e-d87cfa249060"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 245MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model ready.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1926619860.py:21: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training & Validation"
      ],
      "metadata": {
        "id": "k-3Bn4FWoj9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    print(f\"Validation Loss: {val_loss/len(loader):.4f}\")\n",
        "    print(f\"Validation Accuracy: {correct/total:.4f}\")\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, val_loader, epochs=5):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "\n",
        "        for images, labels in tqdm(train_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with autocast():\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        print(f\"\\nEpoch [{epoch+1}/{epochs}] Train Loss: {train_loss/len(train_loader):.4f}\")\n",
        "        validate(model, val_loader)\n",
        "\n",
        "        # Safety checkpoint\n",
        "        if (epoch+1) % 2 == 0:\n",
        "            torch.save(model.state_dict(), os.path.join(MODEL_DIR, \"temp_checkpoint.pth\"))"
      ],
      "metadata": {
        "id": "vWgUw8HIokpb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train"
      ],
      "metadata": {
        "id": "n5tO7f6Top6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, train_loader, val_loader, epochs=5)"
      ],
      "metadata": {
        "id": "WKU3WtX8R7fU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation"
      ],
      "metadata": {
        "id": "bbenC4Zqos3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, loader):\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    fake_confidences = []\n",
        "    real_confidences = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images = images.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "            preds = torch.argmax(probs, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.numpy())\n",
        "\n",
        "            for i in range(len(labels)):\n",
        "                if labels[i] == 1:\n",
        "                    fake_confidences.append(probs[i,1].item())\n",
        "                else:\n",
        "                    real_confidences.append(probs[i,0].item())\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds)\n",
        "    recall = recall_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds)\n",
        "\n",
        "    print(\"Accuracy:\", acc)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1 Score:\", f1)\n",
        "    print(\"Average confidence (Real):\", np.mean(real_confidences))\n",
        "    print(\"Average confidence (Fake):\", np.mean(fake_confidences))\n",
        "\n",
        "    return all_labels, all_preds, fake_confidences, real_confidences"
      ],
      "metadata": {
        "id": "aN6Q8QZyo0dW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels, preds, fake_conf, real_conf = evaluate_model(model, test_loader)"
      ],
      "metadata": {
        "id": "3U5pXXWjo49v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confusion Matrix"
      ],
      "metadata": {
        "id": "lAJyCmIho6X6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(labels, preds)\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=[\"Real\", \"Fake\"],\n",
        "            yticklabels=[\"Real\", \"Fake\"])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jz19XDJio8ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confidence Distribution"
      ],
      "metadata": {
        "id": "zbw4BSSspEVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(fake_conf, bins=30, alpha=0.7, label=\"Fake Confidence\")\n",
        "plt.hist(real_conf, bins=30, alpha=0.7, label=\"Real Confidence\")\n",
        "plt.legend()\n",
        "plt.title(\"Confidence Distribution\")\n",
        "plt.xlabel(\"Confidence\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yPO_h1qho5pC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save Final Model"
      ],
      "metadata": {
        "id": "gR3JyLHbpH3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_PATH = os.path.join(MODEL_DIR, \"resnet18_synthetic_detector.pth\")\n",
        "torch.save(model.state_dict(), MODEL_PATH)\n",
        "\n",
        "print(f\"Model saved at {MODEL_PATH}\")"
      ],
      "metadata": {
        "id": "XluAfPiPpKHe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}